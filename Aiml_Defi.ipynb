{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJQe4MHsoCzB8ggS/xOZlQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeshwanth-A/aiml_defi/blob/main/Aiml_Defi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYVWeqAuN0Oh"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CRYPTO SENTIMENT FORECASTER - COMPLETE CODE FOR GOOGLE COLAB\n",
        "# ============================================================================\n",
        "# Copy this entire code into a single Colab cell and run!\n",
        "# First run: ~10 minutes (trains everything)\n",
        "# Subsequent runs: ~30 seconds (loads from disk)\n",
        "# ============================================================================\n",
        "\n",
        "# Fix for notebook widget metadata error (MUST BE FIRST!)\n",
        "\n",
        "import os, warnings\n",
        "os.environ['JUPYTER_WIDGETS_ENABLED'] = 'false'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from IPython import get_ipython\n",
        "if get_ipython():\n",
        "    get_ipython().run_line_magic('config', \"InlineBackend.figure_formats = ['png']\")\n",
        "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "print(\"🚀 Starting Crypto Sentiment Forecaster...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 0: Install Required Packages\n",
        "# ============================================================================\n",
        "print(\"\\n📦 Installing packages (this may take 2-3 minutes on first run)...\")\n",
        "\n",
        "!pip install -q numpy pandas torch torchvision torchaudio\n",
        "!pip install -q tensorflow\n",
        "!pip install -q scikit-learn matplotlib\n",
        "!pip install -q transformers peft datasets accelerate\n",
        "!pip install -q huggingface-hub\n",
        "\n",
        "print(\"✅ Packages installed successfully!\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Import Libraries\n",
        "# ============================================================================\n",
        "import urllib.request\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from datasets import Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Setup Paths\n",
        "# ============================================================================\n",
        "save_folder = '/content/drive/MyDrive/crypto_project'\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "pt_model_path = os.path.join(save_folder, 'model_pt.pth')\n",
        "tf_model_path = os.path.join(save_folder, 'model_tf.h5')\n",
        "llm_path = os.path.join(save_folder, 'finetuned_llm')\n",
        "data_path = os.path.join(save_folder, 'processed_data.csv')\n",
        "\n",
        "print(f\"\\n📁 Save folder: {save_folder}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Data Fetching Functions\n",
        "# ============================================================================\n",
        "def fetch_and_parse(token_id='uniswap', days=30):\n",
        "    \"\"\"Fetch crypto price data from CoinGecko API\"\"\"\n",
        "    url = f\"https://api.coingecko.com/api/v3/coins/{token_id}/market_chart?vs_currency=usd&days={days}&interval=daily\"\n",
        "    try:\n",
        "        print(f\"   Fetching {days} days of {token_id} data from CoinGecko...\")\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            data = response.read().decode(\"utf-8\")\n",
        "            json_data = json.loads(data)\n",
        "\n",
        "        required_keys = ['prices', 'total_volumes']\n",
        "        for key in required_keys:\n",
        "            if key not in json_data:\n",
        "                print(f\"   ⚠️ Warning: {key} is missing from API response\")\n",
        "                return None\n",
        "\n",
        "        prices = json_data.get('prices', [])\n",
        "        volumes = json_data.get('total_volumes', [])\n",
        "\n",
        "        if len(prices) == 0 or len(volumes) == 0:\n",
        "            print(\"   ⚠️ No data received from API\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(prices, columns=['timestamp', 'price'])\n",
        "        df['volume'] = [v[1] for v in volumes]\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "\n",
        "        print(f\"   ✅ Fetched {len(df)} data points\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Exception fetching data: {e}\")\n",
        "        return None\n",
        "\n",
        "def add_sentiment(df):\n",
        "    \"\"\"Add mock sentiment data (replace with real API in production)\"\"\"\n",
        "    np.random.seed(42)\n",
        "    df['sentiment'] = np.random.uniform(-1, 1, size=len(df))\n",
        "    return df\n",
        "\n",
        "def create_sequences(data, seq_len=10):\n",
        "    \"\"\"Create sequences for time series prediction\"\"\"\n",
        "    seq_len = min(seq_len, len(data) - 1)\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_len):\n",
        "        X.append(data.iloc[i:i+seq_len].values)\n",
        "        y.append(data.iloc[i+seq_len]['price'])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: PyTorch LSTM Model\n",
        "# ============================================================================\n",
        "class PricePredictor(nn.Module):\n",
        "    \"\"\"PyTorch LSTM model for price prediction\"\"\"\n",
        "    def __init__(self, input_size=5, hidden_size=50, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Data Processing\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📊 STEP 1: DATA PROCESSING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    print(\"✅ Loading processed data from disk...\")\n",
        "    df = pd.read_csv(data_path)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    print(f\"   Loaded {len(df)} rows\")\n",
        "else:\n",
        "    print(\"🔄 Fetching and processing fresh data...\")\n",
        "    df = fetch_and_parse(token_id='uniswap', days=30)\n",
        "\n",
        "    if df is not None:\n",
        "        # Clean data\n",
        "        df = df.dropna()\n",
        "\n",
        "        # Add sentiment\n",
        "        df = add_sentiment(df)\n",
        "        print(\"   ✅ Added sentiment features\")\n",
        "\n",
        "        # Calculate volatility using NumPy\n",
        "        prices_np = df['price'].values\n",
        "        window_size = min(5, len(prices_np))\n",
        "\n",
        "        if len(prices_np) >= window_size:\n",
        "            volatility = np.std(\n",
        "                np.lib.stride_tricks.sliding_window_view(prices_np, window_size),\n",
        "                axis=1\n",
        "            )\n",
        "            df['volatility'] = np.pad(volatility, (window_size-1, 0), mode='edge')\n",
        "        else:\n",
        "            df['volatility'] = 0\n",
        "\n",
        "        print(\"   ✅ Calculated volatility\")\n",
        "\n",
        "        # Scale features\n",
        "        scaler = MinMaxScaler()\n",
        "        df[['price', 'volume', 'sentiment', 'volatility']] = scaler.fit_transform(\n",
        "            df[['price', 'volume', 'sentiment', 'volatility']]\n",
        "        )\n",
        "        print(\"   ✅ Scaled features\")\n",
        "\n",
        "        # Create lag features\n",
        "        df['price_lag1'] = df['price'].shift(1)\n",
        "        df = df.dropna()\n",
        "\n",
        "        # Save processed data\n",
        "        df.to_csv(data_path, index=False)\n",
        "        print(f\"   ✅ Saved {len(df)} processed rows\")\n",
        "    else:\n",
        "        print(\"   ❌ Failed to fetch data. Exiting.\")\n",
        "        df = None\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Model Training Pipeline\n",
        "# ============================================================================\n",
        "if df is not None and len(df) > 0:\n",
        "    print(f\"\\n✅ Dataset ready: {len(df)} rows\")\n",
        "\n",
        "    # Create sequences\n",
        "    features = df[['price', 'volume', 'sentiment', 'volatility', 'price_lag1']]\n",
        "    seq_length = min(10, len(features) // 2)\n",
        "    X, y = create_sequences(features, seq_len=seq_length)\n",
        "\n",
        "    if len(X) == 0:\n",
        "        print(\"❌ Not enough data to create sequences. Need more historical data.\")\n",
        "    else:\n",
        "        print(f\"✅ Created {len(X)} sequences of length {seq_length}\")\n",
        "\n",
        "        # Split data\n",
        "        split = max(1, int(0.8 * len(X)))\n",
        "        X_train, X_test = X[:split], X[split:]\n",
        "        y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "        print(f\"   Train: {len(X_train)} samples | Test: {len(X_test)} samples\")\n",
        "\n",
        "        # ====================================================================\n",
        "        # MODEL 1: Random Forest Baseline\n",
        "        # ====================================================================\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"🌳 STEP 2: RANDOM FOREST BASELINE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if len(X_train) > 0:\n",
        "            rf = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=10)\n",
        "            rf.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
        "            rf_preds = rf.predict(X_test.reshape(X_test.shape[0], -1))\n",
        "            rf_mae = mean_absolute_error(y_test, rf_preds)\n",
        "            print(f\"✅ Random Forest MAE: {rf_mae:.4f}\")\n",
        "\n",
        "        # ====================================================================\n",
        "        # MODEL 2: PyTorch LSTM\n",
        "        # ====================================================================\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"🔥 STEP 3: PYTORCH LSTM\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        model_pt = PricePredictor(input_size=X_train.shape[2])\n",
        "\n",
        "        if os.path.exists(pt_model_path):\n",
        "            model_pt.load_state_dict(torch.load(pt_model_path))\n",
        "            print(\"✅ Loaded PyTorch model from disk\")\n",
        "        else:\n",
        "            print(\"🔄 Training PyTorch LSTM (50 epochs)...\")\n",
        "            optimizer = torch.optim.Adam(model_pt.parameters(), lr=0.001)\n",
        "            criterion = nn.MSELoss()\n",
        "            X_train_pt = torch.tensor(X_train, dtype=torch.float32)\n",
        "            y_train_pt = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "            for epoch in range(50):\n",
        "                model_pt.train()\n",
        "                outputs = model_pt(X_train_pt)\n",
        "                loss = criterion(outputs, y_train_pt)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if (epoch + 1) % 10 == 0:\n",
        "                    print(f\"   Epoch {epoch + 1}/50 - Loss: {loss.item():.4f}\")\n",
        "\n",
        "            torch.save(model_pt.state_dict(), pt_model_path)\n",
        "            print(\"✅ Saved PyTorch model\")\n",
        "\n",
        "        # Evaluate PyTorch model\n",
        "        model_pt.eval()\n",
        "        with torch.no_grad():\n",
        "            X_test_pt = torch.tensor(X_test, dtype=torch.float32)\n",
        "            preds_pt = model_pt(X_test_pt).numpy()\n",
        "\n",
        "        pt_mae = mean_absolute_error(y_test, preds_pt)\n",
        "        print(f\"✅ PyTorch LSTM MAE: {pt_mae:.4f}\")\n",
        "\n",
        "        # ====================================================================\n",
        "        # MODEL 3: TensorFlow LSTM\n",
        "        # ====================================================================\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"🧠 STEP 4: TENSORFLOW LSTM\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if os.path.exists(tf_model_path):\n",
        "            print(\"🔄 Loading TensorFlow model from disk...\")\n",
        "            try:\n",
        "                model_tf = tf.keras.models.load_model(\n",
        "                    tf_model_path,\n",
        "                    custom_objects={'mse': tf.keras.losses.MeanSquaredError()}\n",
        "                )\n",
        "                print(\"✅ Loaded TensorFlow model\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error loading model: {e}\")\n",
        "                print(\"   Deleting and retraining...\")\n",
        "                os.remove(tf_model_path)\n",
        "                model_tf = None\n",
        "        else:\n",
        "            model_tf = None\n",
        "\n",
        "        if model_tf is None:\n",
        "            print(\"🔄 Training TensorFlow LSTM (50 epochs)...\")\n",
        "            model_tf = Sequential([\n",
        "                LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "                Dense(1)\n",
        "            ])\n",
        "            model_tf.compile(\n",
        "                optimizer='adam',\n",
        "                loss=tf.keras.losses.MeanSquaredError()\n",
        "            )\n",
        "\n",
        "            history = model_tf.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=50,\n",
        "                batch_size=max(1, len(X_train)//10),\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            model_tf.save(tf_model_path)\n",
        "            print(\"✅ Saved TensorFlow model\")\n",
        "\n",
        "        preds_tf = model_tf.predict(X_test, verbose=0)\n",
        "        tf_mae = mean_absolute_error(y_test, preds_tf)\n",
        "        print(f\"✅ TensorFlow LSTM MAE: {tf_mae:.4f}\")\n",
        "\n",
        "        # ====================================================================\n",
        "        # MODEL 4: LLM Finetuning with LoRA\n",
        "        # ====================================================================\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"🤖 STEP 5: LLM FINETUNING (LoRA)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Create training data for explanation model\n",
        "        explanation_data = {\n",
        "            'text': [\n",
        "                \"The prediction was close to actual value, showing good model performance.\",\n",
        "                \"Large prediction error indicates model needs improvement.\",\n",
        "                \"Price increased but prediction was lower, missing upward trend.\",\n",
        "                \"Sentiment was positive and price rose as expected.\"\n",
        "            ],\n",
        "            'label': [1, 0, 0, 1]\n",
        "        }\n",
        "        dataset = Dataset.from_dict(explanation_data)\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "        def preprocess(examples):\n",
        "            return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "        dataset = dataset.map(preprocess, batched=True)\n",
        "        dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "        if os.path.exists(os.path.join(llm_path, 'adapter_config.json')):\n",
        "            print(\"✅ Loading finetuned LLM...\")\n",
        "            base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                \"distilbert-base-uncased\", num_labels=2\n",
        "            )\n",
        "            model_llm = PeftModel.from_pretrained(base_model, llm_path)\n",
        "            print(\"✅ Loaded finetuned LLM\")\n",
        "        else:\n",
        "            print(\"🔄 Finetuning LLM with LoRA (3 epochs)...\")\n",
        "            lora_config = LoraConfig(\n",
        "                r=8,\n",
        "                lora_alpha=32,\n",
        "                target_modules=[\"q_lin\", \"v_lin\"],\n",
        "                lora_dropout=0.1,\n",
        "                bias=\"none\",\n",
        "                task_type=\"SEQ_CLS\"\n",
        "            )\n",
        "            base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                \"distilbert-base-uncased\", num_labels=2\n",
        "            )\n",
        "            model_llm = get_peft_model(base_model, lora_config)\n",
        "\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=\"./results\",\n",
        "                num_train_epochs=3,\n",
        "                per_device_train_batch_size=2,\n",
        "                save_strategy=\"epoch\",\n",
        "                logging_steps=10,\n",
        "                report_to=\"none\"\n",
        "            )\n",
        "            trainer = Trainer(\n",
        "                model=model_llm,\n",
        "                args=training_args,\n",
        "                train_dataset=dataset\n",
        "            )\n",
        "            trainer.train()\n",
        "            model_llm.save_pretrained(llm_path)\n",
        "            print(\"✅ Saved finetuned LLM\")\n",
        "\n",
        "        # ====================================================================\n",
        "        # STEP 7: Visualizations\n",
        "        # ====================================================================\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"📈 STEP 6: GENERATING VISUALIZATIONS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "        # Plot 1: Predictions vs Actual\n",
        "        plt.subplot(2, 2, 1)\n",
        "        test_indices = range(len(y_test))\n",
        "        plt.plot(test_indices, y_test, 'g-', label='Actual', linewidth=2.5, marker='o', markersize=4)\n",
        "        plt.plot(test_indices, preds_pt, 'b--', label='PyTorch LSTM', alpha=0.8, linewidth=2)\n",
        "        plt.plot(test_indices, preds_tf, 'r--', label='TensorFlow LSTM', alpha=0.8, linewidth=2)\n",
        "        plt.xlabel('Test Sample Index', fontsize=11)\n",
        "        plt.ylabel('Normalized Price', fontsize=11)\n",
        "        plt.title('Model Predictions vs Actual Price', fontsize=13, fontweight='bold')\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Prediction Errors\n",
        "        plt.subplot(2, 2, 2)\n",
        "        pt_errors = np.abs(y_test - preds_pt.flatten())\n",
        "        tf_errors = np.abs(y_test - preds_tf.flatten())\n",
        "        rf_errors = np.abs(y_test - rf_preds)\n",
        "        plt.plot(test_indices, pt_errors, 'b-', label='PyTorch Error', alpha=0.7, linewidth=2)\n",
        "        plt.plot(test_indices, tf_errors, 'r-', label='TensorFlow Error', alpha=0.7, linewidth=2)\n",
        "        plt.plot(test_indices, rf_errors, 'g-', label='Random Forest Error', alpha=0.5, linewidth=1.5)\n",
        "        plt.xlabel('Test Sample Index', fontsize=11)\n",
        "        plt.ylabel('Absolute Error', fontsize=11)\n",
        "        plt.title('Prediction Errors Comparison', fontsize=13, fontweight='bold')\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: Feature Trends\n",
        "        plt.subplot(2, 2, 3)\n",
        "        recent_data = df.tail(len(y_test))\n",
        "        plt.plot(recent_data['sentiment'].values, label='Sentiment', alpha=0.8, linewidth=2, color='purple')\n",
        "        plt.plot(recent_data['volatility'].values, label='Volatility', alpha=0.8, linewidth=2, color='orange')\n",
        "        plt.xlabel('Time Index', fontsize=11)\n",
        "        plt.ylabel('Normalized Value', fontsize=11)\n",
        "        plt.title('Sentiment & Volatility Trends', fontsize=13, fontweight='bold')\n",
        "        plt.legend(fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 4: Model Performance Comparison\n",
        "        plt.subplot(2, 2, 4)\n",
        "        models = ['Random\\nForest', 'PyTorch\\nLSTM', 'TensorFlow\\nLSTM']\n",
        "        maes = [rf_mae, pt_mae, tf_mae]\n",
        "        colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
        "        bars = plt.bar(models, maes, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, mae in zip(bars, maes):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{mae:.4f}',\n",
        "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "        plt.ylabel('Mean Absolute Error', fontsize=11)\n",
        "        plt.title('Model Performance Comparison', fontsize=13, fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        viz_path = os.path.join(save_folder, 'forecast_viz.png')\n",
        "        plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"✅ Saved visualization to: {viz_path}\")\n",
        "        plt.show()\n",
        "        plt.close('all')\n",
        "\n",
        "        # ====================================================================\n",
        "        # FINAL SUMMARY\n",
        "        # ====================================================================\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"🎉 PROJECT COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"\\n📁 All files saved to: {save_folder}\")\n",
        "        print(f\"\\n📊 Model Performance (Lower is Better):\")\n",
        "        print(f\"   • Random Forest MAE:    {rf_mae:.4f}\")\n",
        "        print(f\"   • PyTorch LSTM MAE:     {pt_mae:.4f}\")\n",
        "        print(f\"   • TensorFlow LSTM MAE:  {tf_mae:.4f}\")\n",
        "\n",
        "        best_model = min([('Random Forest', rf_mae), ('PyTorch LSTM', pt_mae), ('TensorFlow LSTM', tf_mae)], key=lambda x: x[1])\n",
        "        print(f\"\\n🏆 Best Model: {best_model[0]} (MAE: {best_model[1]:.4f})\")\n",
        "\n",
        "        print(f\"\\n💾 Saved Files:\")\n",
        "        print(f\"   • Processed Data:  {data_path}\")\n",
        "        print(f\"   • PyTorch Model:   {pt_model_path}\")\n",
        "        print(f\"   • TensorFlow Model: {tf_model_path}\")\n",
        "        print(f\"   • Finetuned LLM:   {llm_path}\")\n",
        "        print(f\"   • Visualization:   {viz_path}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"✅ Run this code again - it will load from disk in ~30 seconds!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "else:\n",
        "    print(\"\\n❌ Error: No data available. Cannot proceed with training.\")\n",
        "    print(\"   Please check your internet connection and try again.\")\n",
        "\n",
        "print(\"\\n🏁 Script execution completed!\")"
      ]
    }
  ]
}